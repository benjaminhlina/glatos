---
title: "Estimate Detection Range for 
                      Acoustic Telemetry Receivers"
date: "Updated: `r Sys.Date()`"
output: 
  rmarkdown::html_document:
    toc: true
    toc_depth: 3
    number_sections: true
    toc_float: true
    toc_collapsed: false
vignette: >
  %\VignetteIndexEntry{Estimate Detection Range for 
                      Acoustic Telemetry Receivers}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

#set 'str' options to desired output format
str_opts <- getOption("str") #get list of options
str_opts$strict.width = "wrap"
str_opts$vec.len = 1
options(str = str_opts)

#set 'width'
options(width = 85)
```

\pagebreak
# Objectives 

This vignette describes methods to quickly and easily calculate the distance 
that a given percentage (e.g., 50%) of detections are heard by a receiver to 
estimate detection range for further long-term deployment and estiamtes of detection efficency. 

The preliminary study design/protocol is the following:

1) Deploy range tags (e.g., min delay 840, max delay 960; continuous tags or tags with other delays work as well) at set distances (e.g., 100, 250, 500, and 750 m) from the receiver you are wanting to range test for 24 hr. 

2) After 24 hr retrieve transmitters and receivers. 

3) Import vrl files, receiver locations, and tag location data into [Fathom Central](https://fathomcentral.com/signin). Unfortunately you will need internet to do this. If you are unable to use internet you can download an older version of the detection efficiency software produced by Innovasea and import the vrl and receiver and tag location data. The manual for this software is quite good and will walk you through how to get your data into the detection efficiency tool. You can download the older version at the following [Innovasea website](https://support.vemco.com/s/downloads). 

4) Highly suggest doing multiple 24 hr deployments at the same distances and/or drifts depending on the range transmitter you have. If you have multiple sets of detection efficiency you will be able to create a model that fits the data better ultimately improving the placement of the tag  

5) Calculate the detection efficiency for each distance over the 24 hr using detection range calculator either linked above or in fathom central and export the csv. 

6) Use the exported csv to estimate the detection efficiency at a given distance using using `detection_range_model()` from `{glatos}`. The function, `detection_range_model()`, will return an estimate of distance for a given  detection effiency (e.g., 50%). 

7) We can use R, Python, or GIS to create deployment latitude and longitude for range tags to be deployed at the distance the model estimates will produce the desired detection efficiency (e.g., 50%). 

8) We can easily redeploy our range tags at the distances the model estimates will produce the desired detection efficiency for a  duration (e.g., 6 month, 1 year), to determine changes in detection efficiency over the study period.  

The code below will walk through how to use the detection efficiency data produced by Innovasea software to estimate the distance and create the redeployment location. 

# Intial deploy 

We will initially deploy range tags at set distances away from the receiver. The below workflow will allow you to determine the latitude and longitude of each tag deployment and export as a gpx file to easily upload to a gps or sonar unit. 

## Load packages and data 
We will first load the desired packages in R. We will load [{dplyr}](https://dplyr.tidyverse.org/) for data manipulation processes,[{ggplot2}](https://ggplot2.tidyverse.org/) to plot our models, [{glatos}](https://github.com/ocean-tracking-network/glatos) to use multiple functions associated with acoustic telemetry, and [{sf}](https://r-spatial.github.io/sf/) to work with spatial data including creating the redeployment locations. 
```{r load packages, message = FALSE}
# ---- Bring in R packages ---- 
{
  library(dplyr)
  library(ggplot2)
  library(glatos)
  library(mapview)
  library(sf)
}
```

We will work with the example data set in [{glatos}](https://github.com/ocean-tracking-network/glatos) but you will want to have a shapefile or sf object of the body of water you're working on and the receiver deployment data. We will download the zip file of Lake Huron from the [USGS repository](https://www.sciencebase.gov/catalog/item/530f8a0ee4b0e7e46bd300dd). You will have to unzip the file local to then import it into R. [{sf}](https://r-spatial.github.io/sf/) does have the ability to extract shapefiles using a URL, however, the URL for Lake Huron is not easy to get find. 

<!-- ```{r} -->
<!-- #get path to example receiver_locations file -->
<!-- rec_file <- system.file("extdata",  -->
<!--   "sample_receivers.csv", package = "glatos") -->

<!-- #note that code above is needed to find the example file -->
<!-- #for real glatos data, use something like below -->
<!-- #rec_file <- "c:/path_to_file/GLATOS_receiverLocations_20150321_132242.csv"    -->

<!-- rcv <- read_glatos_receivers(rec_file) -->

<!-- glimpse(rcv) -->

<!-- rcv_sf <- rcv %>%  -->
<!--   st_as_sf(coords = c("deploy_long", "deploy_lat"),  -->
<!--            crs = 4326) -->


<!-- rcv_osc <- rcv_sf %>%  -->
<!--   filter(glatos_array %in% "OSC") -->


<!-- rcv_osc_12 <- rcv_osc %>%  -->
<!--   filter(station_no %in% 12) -->


<!-- n <- 20 -->

<!-- N = 40 -->
<!-- pts_rep <- rcv_osc_12  -->
<!-- for (n in 2:N) pts_rep <- rbind(pts_rep,rcv_osc_12) -->
<!-- dists <- c(100, 250, 500, 750) -->
<!-- dists <- rep(dists, n) -->

<!-- # apply buffer -->
<!-- buff1 <- st_buffer(pts_rep, dists) %>% st_cast("LINESTRING") -->

<!-- mapview(rcv_osc) +  -->
<!--   mapview(buff1) -->



<!-- # we will do range testing on receiver station no 12  -->
<!-- #  -->
<!-- #  -->
<!-- #  -->
<!-- # lake_huron <- st_read(dsn = "..\\data-raw\\shpfile\\.", -->
<!-- #                       layer = "hydro_p_LakeHuron") %>%  -->
<!-- #   st_transform(crs = 4326) -->
<!-- # ggplot() + -->
<!-- # geom_sf(data = lake_huron) + -->
<!-- #   geom_sf(data = rcv_sf %>% -->
<!-- #             filter(glatos_array %in% "SBI"), aes(colour = glatos_array)) -->
<!-- #   geom_sf(data = rcv_sf, aes(colour = glatos_array))  -->

<!-- ``` -->


# Analysis

For the analysis you will want to start a new script, hence why I have left load packages below. 

## Load packages and data 
We will first load the desired packages in R. We will load [{dplyr}](https://dplyr.tidyverse.org/) for data manipulation processes,[{ggplot2}](https://ggplot2.tidyverse.org/) to plot our models, [{glatos}](https://github.com/ocean-tracking-network/glatos) to use multiple functions associated with acoustic telemetry, and [{sf}](https://r-spatial.github.io/sf/) to work with spatial data including creating the redeployment locations. 
```{r reload, eval = FALSE, message = FALSE}
# ---- Bring in R packages ---- 
{
  library(dplyr)
  library(ggplot2)
  library(glatos)
  library(sf)
}
```

Next we will bring in our example data which is loaded with `{glatos}` but you will need to replace `sample_detection_efficiency` with your data frame either by loading the csv produced by Innovasea software. You can do this multiple ways, I prefer using readr::read_csv() but base R works perfectly fine.  

```{r, results='hide'}
#| title: evaluate data 

# uncomment the lines below to bring in your data and replace with 
# the file path and name of detection efficiency 
# file (replace "YOUR_DET_EFF.csv")
# 
# det_eff <- readr::read_csv("YOUR_DET_EFF.csv")
#
# glimpse(det_eff)

# view sample detection efficiency data 

sample_detection_efficiency

glimpse(sample_detection_efficiency)
```

## Calculate distances

Next we will use `detection_range_model()` to produce estimated distances for particular percentage (e.g., 50%). You will want to look through the help page for the function to make sure you're setting up the model correctly. 

Few additional tips: 

1) With fewer data points a third order polynomial often fits the data better, however, this does not mean that neither a logit or probit model should not be assessed as well.

2) If a third order polynomial model is selected, the formula call 
can be in two different formats. The preferred and default format is
`y ~ -1 + x + I(x ^ 2) + I(x ^ 3) + offset(y-intercept)`, therefore, `model_frame` argument needs to be set `"data_frame"`, which is the default, to properly extract
parameters and determine distances from a receiver for the percentage 
of interest. If using the `base::poly()` in the formula such as, 
`y ~ -1 + poly(x, 3, raw = TRUE) + offset(y-intercept)`, then, `model_frame` argument needs to be set to `"matrix"`.
Both formula formats have `offset()` which sets the  y-intercept. 
The y-intercept needs to be set to 100, as x needs to equal 0 m from a receiver 
because you expect to hear a tag 100% of the time.

3) A third order polynomial will handle preliminary detection efficiency 
percentages (y variable) as whole numbers as the model is not bound by 
0 and 1. While both logit and probit models have to use percentages as 
decimals as the models are bound by 0 and 1. 

First, we will use a third order polynomial.  
```{r}
#| title: use detection_range_model

# third order polynomial: ave_percent is a whole number
m <- detection_range_model(avg_percent ~ -1 + distance_m + I(distance_m ^ 2) + 
                             I(distance_m ^ 3) + offset(intercept), 
                           data = sample_detection_efficiency, 
                           percentage = c(10, 50, 90), 
                           link = "polynomial", 
                           model_frame = "data_frame")
```

Second, we will model the same data using a logit and probit model.  
```{r, warning=FALSE}
#| title: use detection_range_model

# logit model: aver percent is in decimal form

m1 <- detection_range_model(avg_percent_d ~ distance_m, 
                            data = sample_detection_efficiency, 
                            percentage = c(10, 50, 90), 
                            link = "logit",
                            summary_stats = TRUE)

# probit model: aver percent is in decimal form

m2 <- detection_range_model(avg_percent_d ~ distance_m, 
                            data = sample_detection_efficiency, 
                            percentage = c(10, 50, 90), 
                            link = "probit",
                            summary_stats = TRUE)
```

We can will then view each of the results with the first being the third order polynomial. 
```{r}
#| title: view poly
m
```

Second being the logit model. 
```{r}
#| title: view logit 
m1
```

and third the probit model. 
```{r}
#| title: view provit
m2
```

Considering the example data set is quite limited, you will notice each model performs different especially, the third-order polynomial which fits better than the logit and probit models. 

## Plot all three models 

We will first plot the three-order polynominal model first becasue we cannot plot all three models on the same plot. The reason why we can't plot all three models is a scaling issue as the three-order polynominal is bound by 0-100 while the probit and logit models are bound by 0-1. 

```{r, warning = FALSE, message = FALSE}
#| title: plot models 
#| fig-height: 5
#| fig-width: 7
ggplot() +
  geom_point(data = sample_detection_efficiency,
             aes(x = distance_m, y = avg_percent), 
             size = 3) +
  geom_hline(yintercept = c(10, 50, 90), linetype = 2) + 
  geom_smooth(data = sample_detection_efficiency,
              aes(x = distance_m, y = avg_percent),
              method = "lm",
              linewidth = 1,
              formula = y ~ -1 + x + I(x ^ 2) +
                I(x ^ 3),
              method.args = list(offset = sample_detection_efficiency$intercept),
              colour = "#8da0cb", se = FALSE) +
  scale_y_continuous(breaks = seq(0, 100, 20)) + 
  theme_bw(base_size = 15) +
  theme(
    panel.grid = element_blank()
  ) +
  labs(
    x = "Distance (m)", 
    y = "Detection efficency (%)"
  )
```

I have added in the dotted lines at where the 10, 50, and 90% detection efficiency occurs. We will then plot the logit and probit models next. 

```{r, warning = FALSE, message = FALSE}
#| title: plot logist and probit models 
#| fig-height: 5
#| fig-width: 7
ggplot() +
  geom_point(data = sample_detection_efficiency,
             aes(x = distance_m, y = avg_percent_d), 
             size = 3) +
  geom_hline(yintercept = c(0.10, 0.50, 0.90), linetype = 2) +
  geom_smooth(data = sample_detection_efficiency,
              aes(x = distance_m, y = avg_percent_d),
              method = "glm",
              linewidth = 1,
              method.args = list(family = binomial(link = "logit")),
              colour = "#66c2a5", se = FALSE) +
  geom_smooth(data = sample_detection_efficiency,
              aes(x = distance_m, y = avg_percent_d),
              method = "glm",
              linewidth = 1,
              method.args = list(family = binomial(link = "probit")),
              colour = "#fc8d62", se = FALSE) + 
  scale_y_continuous(breaks = seq(0, 1, 0.20)) + 
  theme_bw(base_size = 15) +
  theme(
    panel.grid = element_blank()
  ) +
  labs(
    x = "Distance (m)", 
    y = "Detection efficency (%)"
  )
```

# Reploy 